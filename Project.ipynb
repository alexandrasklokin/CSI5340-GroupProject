{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code cell intended for imports and global settings.\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from SupportClasses import ModelSupport as ms, EnvironmentSetup as env\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Global values\n",
    "NUMBER_OF_NODES = 3     # We only support values between 2 and 4 at present.\n",
    "NUMBER_OF_EPOCHS = 5\n",
    "NUMBER_OF_CLASSES = NUMBER_OF_NODES     # Generally we will have the same number of classes as nodes.\n",
    "PATH = 'baseline-model.pt'\n",
    "DATASET = 'cifar10'     # You can either pick: mnist, fashion_mnist, or cifar10.\n",
    "SUBSET_MAX_SIZE = 100   # This value controls the maximum size of a training subset. This can be modified.\n",
    "\n",
    "# Model specific values\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Remove baseline and best model when we restart\n",
    "os.remove(\"baseline-model.pt\")\n",
    "os.remove(\"best-model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "The classes in the training and testing set are ['airplane', 'automobile', 'bird']\n",
      "Done importing data.\n"
     ]
    }
   ],
   "source": [
    "# This code cell will be used for setting up the unbalanced datasets.\n",
    "\n",
    "# Note that we are implicitly assuming the data is well balanced in the original dataset.\n",
    "# Data distributions based on the number of nodes.\n",
    "data_distribution_list = env.data_distribution(NUMBER_OF_NODES)\n",
    "train_set, validation_set, test_set, classes = env.download_cifar10(NUMBER_OF_CLASSES)\n",
    "\n",
    "# Now we distribute the dataset, for each node.\n",
    "unbalanced_training_sets = []\n",
    "for data_dist in data_distribution_list:\n",
    "    unbalanced_training_sets.append( env.unbalance_training_set(train_set=train_set, classes=classes, data_distribution=data_dist,\n",
    "                                                                subset_max_size=SUBSET_MAX_SIZE) )\n",
    "\n",
    "print(\"Done importing data.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This code cell is likely where you will want to do the GAN work on the given datasets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "ConvNetCifar(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=3, bias=True)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The global model will be trained on the mix of the unbalanced training sets.\n",
    "global_train_loader = env.create_single_loader(ConcatDataset(unbalanced_training_sets))\n",
    "# This code cell is to be used for importing data and setting up the model.\n",
    "training_loaders, validation_loader, test_loader = env.create_data_loaders(training_sets=unbalanced_training_sets,\n",
    "                                                                       validation_set=validation_set, test_set=test_set)\n",
    "# Create and load the models. We initiate the model with None as we will update it with the global model in each round.\n",
    "fed_models = {f\"Federated_Model_{i+1}\": ms.FederatedModel(train_loader, validation_loader,\n",
    "                  ms.ConvNetCifar(NUMBER_OF_CLASSES) if DATASET == 'cifar10' else ms.ConvNetMnist(NUMBER_OF_CLASSES))\n",
    "                for i, train_loader in enumerate(training_loaders)}\n",
    "\n",
    "# Create the baseline, non-federated model.\n",
    "baseline_model = ms.ConvNetCifar(NUMBER_OF_CLASSES) if DATASET == 'cifar10' else ms.ConvNetMnist(NUMBER_OF_CLASSES)\n",
    "# Create the federated model\n",
    "federated_model = ms.ConvNetCifar(NUMBER_OF_CLASSES) if DATASET == 'cifar10' else ms.ConvNetMnist(NUMBER_OF_CLASSES)\n",
    "\n",
    "# Send the models to the CUDA device if it exists.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "baseline_model.to(device=device)\n",
    "federated_model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 4s\n",
      "\tTrain Loss: 0.03163 | Train Acc: 66.00%\n",
      "\t Val. Loss: 0.00866 |  Val. Acc: 63.60%\n",
      "Epoch: 02 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 4s\n",
      "\tTrain Loss: 0.02880 | Train Acc: 71.67%\n",
      "\t Val. Loss: 0.00853 |  Val. Acc: 64.98%\n",
      "Epoch: 03 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 4s\n",
      "\tTrain Loss: 0.02726 | Train Acc: 73.67%\n",
      "\t Val. Loss: 0.00856 |  Val. Acc: 63.91%\n",
      "Epoch: 04 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 4s\n",
      "\tTrain Loss: 0.02541 | Train Acc: 73.33%\n",
      "\t Val. Loss: 0.00853 |  Val. Acc: 63.69%\n",
      "Epoch: 05 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 4s\n",
      "\tTrain Loss: 0.02368 | Train Acc: 75.67%\n",
      "\t Val. Loss: 0.00821 |  Val. Acc: 66.49%\n",
      "Baseline model training complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we train a baseline model on all data. No federation as our baseline.\n",
    "optimizer = optim.Adam(baseline_model.parameters())\n",
    "\n",
    "# We train a new model, if the model does not already exist in memory.\n",
    "if not os.path.exists(PATH):\n",
    "    for epoch in range(NUMBER_OF_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = ms.train(baseline_model, global_train_loader, optimizer, criterion, device=device)\n",
    "        valid_loss, valid_acc = ms.test(baseline_model, validation_loader, criterion, device=device)\n",
    "        end_time = time.time()\n",
    "        # Get the time to perform non-federated learning\n",
    "        epoch_mins, epoch_secs = ms.epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Model name: Baseline Model | Epoch time (Baseline Training): {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    torch.save(baseline_model.state_dict(), PATH)\n",
    "print(\"Baseline model training complete.\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.03636 | Train Acc: 61.00%\n",
      "\t Val. Loss: 0.01130 |  Val. Acc: 40.67%\n",
      "Epoch: 01 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.03952 | Train Acc: 56.00%\n",
      "\t Val. Loss: 0.01128 |  Val. Acc: 38.93%\n",
      "Epoch: 01 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.03906 | Train Acc: 52.00%\n",
      "\t Val. Loss: 0.01044 |  Val. Acc: 42.84%\n",
      "Epoch: 01 | Model name: Federated Average | Epoch time (Federated Training): 0m 14s\n",
      "\t Val. Loss: 0.00989 |  Val. Acc: 58.44%\n",
      "Epoch: 02 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.03464 | Train Acc: 67.00%\n",
      "\t Val. Loss: 0.01064 |  Val. Acc: 46.84%\n",
      "Epoch: 02 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.03820 | Train Acc: 56.00%\n",
      "\t Val. Loss: 0.01094 |  Val. Acc: 42.67%\n",
      "Epoch: 02 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.03746 | Train Acc: 56.00%\n",
      "\t Val. Loss: 0.01034 |  Val. Acc: 45.07%\n",
      "Epoch: 02 | Model name: Federated Average | Epoch time (Federated Training): 0m 13s\n",
      "\t Val. Loss: 0.00949 |  Val. Acc: 60.13%\n",
      "Epoch: 03 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.03370 | Train Acc: 65.00%\n",
      "\t Val. Loss: 0.01013 |  Val. Acc: 48.18%\n",
      "Epoch: 03 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.03841 | Train Acc: 53.00%\n",
      "\t Val. Loss: 0.01092 |  Val. Acc: 45.16%\n",
      "Epoch: 03 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.03615 | Train Acc: 56.00%\n",
      "\t Val. Loss: 0.01056 |  Val. Acc: 46.71%\n",
      "Epoch: 03 | Model name: Federated Average | Epoch time (Federated Training): 0m 14s\n",
      "\t Val. Loss: 0.00928 |  Val. Acc: 61.73%\n",
      "Epoch: 04 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.03269 | Train Acc: 63.00%\n",
      "\t Val. Loss: 0.00999 |  Val. Acc: 48.93%\n",
      "Epoch: 04 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.03855 | Train Acc: 54.00%\n",
      "\t Val. Loss: 0.01173 |  Val. Acc: 38.89%\n",
      "Epoch: 04 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.03554 | Train Acc: 59.00%\n",
      "\t Val. Loss: 0.01027 |  Val. Acc: 50.53%\n",
      "Epoch: 04 | Model name: Federated Average | Epoch time (Federated Training): 0m 14s\n",
      "\t Val. Loss: 0.00932 |  Val. Acc: 61.29%\n",
      "Epoch: 05 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.03089 | Train Acc: 67.00%\n",
      "\t Val. Loss: 0.01139 |  Val. Acc: 45.33%\n",
      "Epoch: 05 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.03669 | Train Acc: 58.00%\n",
      "\t Val. Loss: 0.01015 |  Val. Acc: 54.62%\n",
      "Epoch: 05 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.03386 | Train Acc: 60.00%\n",
      "\t Val. Loss: 0.01027 |  Val. Acc: 47.60%\n",
      "Epoch: 05 | Model name: Federated Average | Epoch time (Federated Training): 0m 14s\n",
      "\t Val. Loss: 0.00902 |  Val. Acc: 59.82%\n",
      "Federated Model training complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we train our federated model.\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    # Perform the computation steps on the individual models\n",
    "    start_time = time.time()\n",
    "    for key, fed_model in fed_models.items():\n",
    "        # Update each model with the global model, before training again.\n",
    "        fed_model.model.load_state_dict(federated_model.state_dict())\n",
    "        fed_model.model.to(device=device)\n",
    "\n",
    "        # Begin training\n",
    "        optimizer = optim.Adam(fed_model.model.parameters())\n",
    "        train_loss, train_acc = ms.train(fed_model.model, fed_model.train_loader, optimizer, criterion, device=device)\n",
    "        valid_loss, valid_acc = ms.test(fed_model.model, fed_model.validation_loader, criterion, device=device)\n",
    "        print(f'Epoch: {epoch+1:02} | Model name: {key}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    end_time = time.time()\n",
    "    # Get the time to perform federated learning\n",
    "    epoch_mins, epoch_secs = ms.epoch_time(start_time, end_time)\n",
    "\n",
    "    # Average the federated models and combine their weights into the main model.\n",
    "    federated_model.load_state_dict(ms.federated_averaging(fed_models))\n",
    "    # Validate this model on a, small balanced validation set\n",
    "    valid_loss, valid_acc = ms.test(federated_model, validation_loader, criterion, device=device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # This will save our best model in case we encounter a drop off during training.\n",
    "        torch.save(federated_model.state_dict(), 'best-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Model name: Federated Average | Epoch time (Federated Training): {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "print(\"Federated Model training complete.\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: Baseline | Test Loss: 0.008 | Test Acc: 67.03%\n",
      "Model name: Federated Average | Test Loss: 0.009 | Test Acc: 59.33%\n"
     ]
    }
   ],
   "source": [
    "# The main testing loop\n",
    "# Load the model\n",
    "baseline_model.load_state_dict(torch.load(PATH))\n",
    "federated_model.load_state_dict(torch.load('best-model.pt'))\n",
    "\n",
    "baseline_test_loss, baseline_test_acc = ms.test(baseline_model, test_loader, criterion, device=device)\n",
    "fed_avg_test_loss, fed_avg_test_acc = ms.test(federated_model, test_loader, criterion, device=device)\n",
    "\n",
    "print(f'Model name: Baseline | Test Loss: {baseline_test_loss:.3f} | Test Acc: {baseline_test_acc*100:.2f}%')\n",
    "print(f'Model name: Federated Average | Test Loss: {fed_avg_test_loss:.3f} | Test Acc: {fed_avg_test_acc*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}