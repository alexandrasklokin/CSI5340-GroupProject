{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code cell intended for imports and global settings.\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from SupportClasses import ModelSupport as ms, EnvironmentSetup as env\n",
    "\n",
    "# Global values\n",
    "NUMBER_OF_NODES = 3     # We only support values between 2 and 4 at present.\n",
    "NUMBER_OF_EPOCHS = 5\n",
    "NUMBER_OF_CLASSES = NUMBER_OF_NODES     # Generally we will have the same number of classes as nodes.\n",
    "PATH = 'baseline-model.pt'\n",
    "\n",
    "# Model specific values\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classes in the training and testing set are [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# This code cell will be used for setting up the unbalanced datasets.\n",
    "\n",
    "# Note that we are implicitly assuming the data is well balanced in the original dataset.\n",
    "# Data distributions based on the number of nodes.\n",
    "data_distribution_list = env.data_distribution(NUMBER_OF_NODES)\n",
    "train_set, validation_set, test_set, classes = env.download_mnist(NUMBER_OF_CLASSES)\n",
    "\n",
    "# Now we distribute the dataset, for each node.\n",
    "unbalanced_training_sets = []\n",
    "for data_dist in data_distribution_list:\n",
    "    unbalanced_training_sets.append( env.unbalance_training_set(train_set=train_set, classes=classes, data_distribution=data_dist) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This code cell is likely where you will want to do the GAN work on the given datasets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "ConvNet(\n  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (fc1): Linear(in_features=1568, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=3, bias=True)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_train_loader = env.create_single_loader(train_set.dataset)\n",
    "# This code cell is to be used for importing data and setting up the model.\n",
    "training_loaders, validation_loader, test_loader = env.create_data_loaders(training_sets=unbalanced_training_sets,\n",
    "                                                                       validation_set=validation_set, test_set=test_set)\n",
    "# Create and load the models. We initiate the model with None as we will update it with the global model in each round.\n",
    "fed_models = {f\"Federated_Model_{i+1}\": ms.FederatedModel(train_loader, validation_loader,\n",
    "                                                          ms.ConvNet(NUMBER_OF_CLASSES))\n",
    "                for i, train_loader in enumerate(training_loaders)}\n",
    "\n",
    "# Create the baseline, non-federated model.\n",
    "baseline_model = ms.ConvNet(NUMBER_OF_CLASSES)\n",
    "# Create the federated model\n",
    "federated_model = ms.ConvNet(NUMBER_OF_CLASSES)\n",
    "\n",
    "# Send the models to the CUDA device if it exists.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "baseline_model.to(device=device)\n",
    "federated_model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18625 5923 6742 5958\n",
      "Epoch: 01 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 9s\n",
      "\tTrain Loss: 0.00158 | Train Acc: 98.81%\n",
      "\t Val. Loss: 0.00011 |  Val. Acc: 99.57%\n",
      "18625 5923 6742 5958\n",
      "Epoch: 02 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 9s\n",
      "\tTrain Loss: 0.00042 | Train Acc: 99.61%\n",
      "\t Val. Loss: 0.00002 |  Val. Acc: 99.89%\n",
      "18625 5923 6742 5958\n",
      "Epoch: 03 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 9s\n",
      "\tTrain Loss: 0.00026 | Train Acc: 99.83%\n",
      "\t Val. Loss: 0.00002 |  Val. Acc: 99.89%\n",
      "18625 5923 6742 5958\n",
      "Epoch: 04 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 9s\n",
      "\tTrain Loss: 0.00026 | Train Acc: 99.80%\n",
      "\t Val. Loss: 0.00001 |  Val. Acc: 100.00%\n",
      "18625 5923 6742 5958\n",
      "Epoch: 05 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 11s\n",
      "\tTrain Loss: 0.00012 | Train Acc: 99.90%\n",
      "\t Val. Loss: 0.00000 |  Val. Acc: 100.00%\n",
      "Baseline model training complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we train a baseline model on all data. No federation as our baseline.\n",
    "optimizer = optim.Adam(baseline_model.parameters())\n",
    "\n",
    "# We train a new model, if the model does not already exist in memory.\n",
    "if not os.path.exists(PATH):\n",
    "    for epoch in range(NUMBER_OF_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = ms.train(baseline_model, global_train_loader, optimizer, criterion, device=device)\n",
    "        valid_loss, valid_acc = ms.test(baseline_model, validation_loader, criterion, device=device)\n",
    "        end_time = time.time()\n",
    "        # Get the time to perform non-federated learning\n",
    "        epoch_mins, epoch_secs = ms.epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Model name: Baseline Model | Epoch time (Baseline Training): {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    torch.save(baseline_model.state_dict(), PATH)\n",
    "print(\"Baseline model training complete.\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400 1264 2846 1271\n",
      "Epoch: 01 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.00334 | Train Acc: 96.88%\n",
      "\t Val. Loss: 0.00027 |  Val. Acc: 99.10%\n",
      "5250 1264 1423 2542\n",
      "Epoch: 01 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.00331 | Train Acc: 97.19%\n",
      "\t Val. Loss: 0.00024 |  Val. Acc: 99.36%\n",
      "5225 2528 1423 1271\n",
      "Epoch: 01 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.00362 | Train Acc: 96.71%\n",
      "\t Val. Loss: 0.00035 |  Val. Acc: 98.68%\n",
      "Epoch: 01 | Model name: Federated Average | Epoch time (Federated Training): 1m 33s\n",
      "\t Val. Loss: 0.00042 |  Val. Acc: 98.64%\n",
      "5400 1264 2846 1271\n",
      "Epoch: 02 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.00115 | Train Acc: 99.09%\n",
      "\t Val. Loss: 0.00021 |  Val. Acc: 99.43%\n",
      "5250 1264 1423 2542\n",
      "Epoch: 02 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.00104 | Train Acc: 99.08%\n",
      "\t Val. Loss: 0.00016 |  Val. Acc: 99.53%\n",
      "5225 2528 1423 1271\n",
      "Epoch: 02 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.00087 | Train Acc: 99.31%\n",
      "\t Val. Loss: 0.00021 |  Val. Acc: 99.32%\n",
      "Epoch: 02 | Model name: Federated Average | Epoch time (Federated Training): 0m 13s\n",
      "\t Val. Loss: 0.00014 |  Val. Acc: 99.57%\n",
      "5400 1264 2846 1271\n",
      "Epoch: 03 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.00063 | Train Acc: 99.50%\n",
      "\t Val. Loss: 0.00017 |  Val. Acc: 99.46%\n",
      "5250 1264 1423 2542\n",
      "Epoch: 03 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.00071 | Train Acc: 99.43%\n",
      "\t Val. Loss: 0.00027 |  Val. Acc: 99.07%\n",
      "5225 2528 1423 1271\n",
      "Epoch: 03 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.00072 | Train Acc: 99.48%\n",
      "\t Val. Loss: 0.00023 |  Val. Acc: 99.28%\n",
      "Epoch: 03 | Model name: Federated Average | Epoch time (Federated Training): 0m 14s\n",
      "\t Val. Loss: 0.00014 |  Val. Acc: 99.53%\n",
      "5400 1264 2846 1271\n",
      "Epoch: 04 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.00057 | Train Acc: 99.59%\n",
      "\t Val. Loss: 0.00020 |  Val. Acc: 99.39%\n",
      "5250 1264 1423 2542\n",
      "Epoch: 04 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.00057 | Train Acc: 99.62%\n",
      "\t Val. Loss: 0.00014 |  Val. Acc: 99.46%\n",
      "5225 2528 1423 1271\n",
      "Epoch: 04 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.00051 | Train Acc: 99.64%\n",
      "\t Val. Loss: 0.00082 |  Val. Acc: 97.57%\n",
      "Epoch: 04 | Model name: Federated Average | Epoch time (Federated Training): 0m 14s\n",
      "\t Val. Loss: 0.00012 |  Val. Acc: 99.57%\n",
      "5400 1264 2846 1271\n",
      "Epoch: 05 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.00052 | Train Acc: 99.70%\n",
      "\t Val. Loss: 0.00034 |  Val. Acc: 99.03%\n",
      "5250 1264 1423 2542\n",
      "Epoch: 05 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.00064 | Train Acc: 99.58%\n",
      "\t Val. Loss: 0.00019 |  Val. Acc: 99.43%\n",
      "5225 2528 1423 1271\n",
      "Epoch: 05 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.00038 | Train Acc: 99.75%\n",
      "\t Val. Loss: 0.00015 |  Val. Acc: 99.53%\n",
      "Epoch: 05 | Model name: Federated Average | Epoch time (Federated Training): 0m 14s\n",
      "\t Val. Loss: 0.00008 |  Val. Acc: 99.79%\n",
      "Federated Model training complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we train our federated model.\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    # Perform the computation steps on the individual models\n",
    "    start_time = time.time()\n",
    "    for key, fed_model in fed_models.items():\n",
    "        # Update each model with the global model, before training again.\n",
    "        fed_model.model.load_state_dict(federated_model.state_dict())\n",
    "        fed_model.model.to(device=device)\n",
    "\n",
    "        # Begin training\n",
    "        optimizer = optim.Adam(fed_model.model.parameters())\n",
    "        train_loss, train_acc = ms.train(fed_model.model, fed_model.train_loader, optimizer, criterion, device=device)\n",
    "        valid_loss, valid_acc = ms.test(fed_model.model, fed_model.validation_loader, criterion, device=device)\n",
    "        print(f'Epoch: {epoch+1:02} | Model name: {key}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    end_time = time.time()\n",
    "    # Get the time to perform federated learning\n",
    "    epoch_mins, epoch_secs = ms.epoch_time(start_time, end_time)\n",
    "\n",
    "    # Average the federated models and combine their weights into the main model.\n",
    "    federated_model.load_state_dict(ms.federated_averaging(fed_models))\n",
    "    # Validate this model on a, small balanced validation set\n",
    "    valid_loss, valid_acc = ms.test(federated_model, validation_loader, criterion, device=device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # This will save our best model in case we encounter a drop off during training.\n",
    "        torch.save(federated_model.state_dict(), 'best-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Model name: Federated Average | Epoch time (Federated Training): {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "print(\"Federated Model training complete.\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: Baseline | Test Loss: 0.000 | Test Acc: 99.90%\n",
      "Model name: Federated Average | Test Loss: 0.000 | Test Acc: 99.94%\n"
     ]
    }
   ],
   "source": [
    "# The main testing loop\n",
    "# Load the model\n",
    "baseline_model.load_state_dict(torch.load(PATH))\n",
    "federated_model.load_state_dict(torch.load('best-model.pt'))\n",
    "\n",
    "baseline_test_loss, baseline_test_acc = ms.test(baseline_model, test_loader, criterion, device=device)\n",
    "fed_avg_test_loss, fed_avg_test_acc = ms.test(federated_model, test_loader, criterion, device=device)\n",
    "\n",
    "print(f'Model name: Baseline | Test Loss: {baseline_test_loss:.3f} | Test Acc: {baseline_test_acc*100:.2f}%')\n",
    "print(f'Model name: Federated Average | Test Loss: {fed_avg_test_loss:.3f} | Test Acc: {fed_avg_test_acc*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}