{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code cell intended for imports and global settings.\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from SupportClasses import ModelSupport as ms, EnvironmentSetup as env\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Global values\n",
    "# These are the values you can change\n",
    "NUMBER_OF_NODES = 3     # We only support values between 2 and 4 at present.\n",
    "NUMBER_OF_EPOCHS = 5\n",
    "DATA_DISTRIBUTION = [None, None, (25, 100), (25, 25, 100), (25, 25, 25, 100)]    # Keep these values under 3000.\n",
    "NUMBER_OF_CLASSES = NUMBER_OF_NODES     # Generally we will have the same number of classes as nodes.\n",
    "DATASET = env.Dataset.CIFAR_10     # You can either pick: MNIST, fashion_mnist, or cifar10.\n",
    "MU, SIGMA = 0, 0        # These values are meant to be used for adding noise to the federated average model.\n",
    "# End of values you can change.\n",
    "\n",
    "BEST_FEDERATED_MODEL_PATH = \"best-model.pt\"\n",
    "BASELINE_MODEL_PATH = 'baseline-model.pt'\n",
    "\n",
    "# Model specific values\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Remove baseline and best model when we restart\n",
    "try:\n",
    "    os.remove(BEST_FEDERATED_MODEL_PATH)\n",
    "    os.remove(BASELINE_MODEL_PATH)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "The classes in the training and testing set are ['airplane', 'automobile']\n",
      "Done importing data.\n"
     ]
    }
   ],
   "source": [
    "# This code cell will be used for setting up the unbalanced datasets.\n",
    "\n",
    "# Note that we are implicitly assuming the data is well balanced in the original dataset.\n",
    "# Data distributions based on the number of nodes.\n",
    "data_distribution_list = env.data_distribution(NUMBER_OF_NODES, DATA_DISTRIBUTION)\n",
    "train_set, validation_set, test_set, classes = env.download_cifar10(NUMBER_OF_CLASSES)\n",
    "\n",
    "# Now we distribute the dataset, for each node.\n",
    "unbalanced_training_sets = []\n",
    "for data_dist in data_distribution_list:\n",
    "    unbalanced_training_sets.append( env.unbalance_training_set(train_set=train_set, classes=classes,\n",
    "                                                                data_distribution=data_dist) )\n",
    "\n",
    "print(\"Done importing data.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# This code cell is likely where you will want to do the GAN work on the given datasets.\n",
    "\n",
    "'''\n",
    "This is roughly how the data will look. The number will be replaced with actual datasets of the size specified in\n",
    "DATA_DISTRIBUTION based on the number of nodes you are currently working with.\n",
    "'''\n",
    "# unbalanced_training_sets = [[class1=15, class2=15, class3=600], [class1=600, class2=15, class3=15], [class1=15, class2=600, class3=15]]\n",
    "\n",
    "'''\n",
    "When leaving this code cell make sure to Concatenate the datasets into the unbalanced_training_sets variable.\n",
    "You can likely leave the following line as is. Though you may have to change it based on the changes you made.\n",
    "'''\n",
    "unbalanced_training_sets = [ConcatDataset(dataset) for dataset in unbalanced_training_sets]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "ConvNetCifar(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=2, bias=True)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The global model will be trained on the mix of the unbalanced training sets.\n",
    "global_train_loader = env.create_single_loader(ConcatDataset(unbalanced_training_sets))\n",
    "# This code cell is to be used for importing data and setting up the model.\n",
    "training_loaders, validation_loader, test_loader = env.create_data_loaders(training_sets=unbalanced_training_sets,\n",
    "                                                                       validation_set=validation_set, test_set=test_set)\n",
    "# Create and load the models. We initiate the model with None as we will update it with the global model in each round.\n",
    "fed_models = {f\"Federated_Model_{i+1}\": ms.FederatedModel(train_loader, validation_loader,\n",
    "                  ms.ConvNetCifar(NUMBER_OF_CLASSES) if DATASET == env.Dataset.CIFAR_10 else ms.ConvNetMnist(NUMBER_OF_CLASSES))\n",
    "                for i, train_loader in enumerate(training_loaders)}\n",
    "\n",
    "# Create the baseline, non-federated model.\n",
    "baseline_model = ms.ConvNetCifar(NUMBER_OF_CLASSES) if DATASET == env.Dataset.CIFAR_10 else ms.ConvNetMnist(NUMBER_OF_CLASSES)\n",
    "# Create the federated model\n",
    "federated_model = ms.ConvNetCifar(NUMBER_OF_CLASSES) if DATASET == env.Dataset.CIFAR_10 else ms.ConvNetMnist(NUMBER_OF_CLASSES)\n",
    "\n",
    "# Send the models to the CUDA device if it exists.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "baseline_model.to(device=device)\n",
    "federated_model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 4s\n",
      "\tTrain Loss: 0.02711 | Train Acc: 51.60%\n",
      "\t Val. Loss: 0.00638 |  Val. Acc: 73.80%\n",
      "Epoch: 02 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 4s\n",
      "\tTrain Loss: 0.02233 | Train Acc: 76.00%\n",
      "\t Val. Loss: 0.00606 |  Val. Acc: 72.33%\n",
      "Epoch: 03 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 4s\n",
      "\tTrain Loss: 0.01998 | Train Acc: 76.80%\n",
      "\t Val. Loss: 0.00530 |  Val. Acc: 74.87%\n",
      "Epoch: 04 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 4s\n",
      "\tTrain Loss: 0.01833 | Train Acc: 79.60%\n",
      "\t Val. Loss: 0.00495 |  Val. Acc: 77.27%\n",
      "Epoch: 05 | Model name: Baseline Model | Epoch time (Baseline Training): 0m 4s\n",
      "\tTrain Loss: 0.01716 | Train Acc: 80.00%\n",
      "\t Val. Loss: 0.00459 |  Val. Acc: 79.07%\n",
      "Baseline model training complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we train a baseline model on all data. No federation as our baseline.\n",
    "optimizer = optim.Adam(baseline_model.parameters())\n",
    "\n",
    "# We train a new model, if the model does not already exist in memory.\n",
    "if not os.path.exists(BASELINE_MODEL_PATH):\n",
    "    for epoch in range(NUMBER_OF_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = ms.train(baseline_model, global_train_loader, optimizer, criterion, device=device)\n",
    "        valid_loss, valid_acc = ms.test(baseline_model, validation_loader, criterion, device=device)\n",
    "        end_time = time.time()\n",
    "        # Get the time to perform non-federated learning\n",
    "        epoch_mins, epoch_secs = ms.epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Model name: Baseline Model | Epoch time (Baseline Training): {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    torch.save(baseline_model.state_dict(), BASELINE_MODEL_PATH)\n",
    "print(\"Baseline model training complete.\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.02625 | Train Acc: 73.60%\n",
      "\t Val. Loss: 0.00753 |  Val. Acc: 49.20%\n",
      "Epoch: 01 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.02432 | Train Acc: 76.00%\n",
      "\t Val. Loss: 0.00733 |  Val. Acc: 50.80%\n",
      "Epoch: 01 | Model name: Federated Average | Epoch time (Federated Training): 0m 8s\n",
      "\t Val. Loss: 0.00688 |  Val. Acc: 70.67%\n",
      "Epoch: 02 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.02525 | Train Acc: 77.60%\n",
      "\t Val. Loss: 0.00854 |  Val. Acc: 49.20%\n",
      "Epoch: 02 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.02205 | Train Acc: 82.40%\n",
      "\t Val. Loss: 0.00886 |  Val. Acc: 50.80%\n",
      "Epoch: 02 | Model name: Federated Average | Epoch time (Federated Training): 0m 8s\n",
      "\t Val. Loss: 0.00674 |  Val. Acc: 66.40%\n",
      "Epoch: 03 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.02498 | Train Acc: 70.40%\n",
      "\t Val. Loss: 0.00889 |  Val. Acc: 49.20%\n",
      "Epoch: 03 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.01896 | Train Acc: 80.80%\n",
      "\t Val. Loss: 0.01102 |  Val. Acc: 50.80%\n",
      "Epoch: 03 | Model name: Federated Average | Epoch time (Federated Training): 0m 8s\n",
      "\t Val. Loss: 0.00659 |  Val. Acc: 61.67%\n",
      "Epoch: 04 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.02435 | Train Acc: 72.80%\n",
      "\t Val. Loss: 0.00968 |  Val. Acc: 49.20%\n",
      "Epoch: 04 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.01850 | Train Acc: 81.60%\n",
      "\t Val. Loss: 0.01107 |  Val. Acc: 50.80%\n",
      "Epoch: 04 | Model name: Federated Average | Epoch time (Federated Training): 0m 8s\n",
      "\t Val. Loss: 0.00653 |  Val. Acc: 73.07%\n",
      "Epoch: 05 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.02284 | Train Acc: 80.00%\n",
      "\t Val. Loss: 0.01252 |  Val. Acc: 49.20%\n",
      "Epoch: 05 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.01957 | Train Acc: 80.00%\n",
      "\t Val. Loss: 0.01065 |  Val. Acc: 50.80%\n",
      "Epoch: 05 | Model name: Federated Average | Epoch time (Federated Training): 0m 8s\n",
      "\t Val. Loss: 0.00628 |  Val. Acc: 72.73%\n",
      "Federated Model training complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we train our federated model.\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    # Perform the computation steps on the individual models\n",
    "    start_time = time.time()\n",
    "    for key, fed_model in fed_models.items():\n",
    "        # Update each model with the global model, before training again.\n",
    "        fed_model.model.load_state_dict(federated_model.state_dict())\n",
    "        fed_model.model.to(device=device)\n",
    "\n",
    "        # Begin training\n",
    "        optimizer = optim.Adam(fed_model.model.parameters())\n",
    "        train_loss, train_acc = ms.train(fed_model.model, fed_model.train_loader, optimizer, criterion, device=device)\n",
    "        valid_loss, valid_acc = ms.test(fed_model.model, fed_model.validation_loader, criterion, device=device)\n",
    "        print(f'Epoch: {epoch+1:02} | Model name: {key}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    end_time = time.time()\n",
    "    # Get the time to perform federated learning\n",
    "    epoch_mins, epoch_secs = ms.epoch_time(start_time, end_time)\n",
    "\n",
    "    # Average the federated models and combine their weights into the main model.\n",
    "    federated_model.load_state_dict(ms.federated_averaging(fed_models, MU, SIGMA))\n",
    "    # Validate this model on a, small balanced validation set\n",
    "    valid_loss, valid_acc = ms.test(federated_model, validation_loader, criterion, device=device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # This will save our best model in case we encounter a drop off during training.\n",
    "        torch.save(federated_model.state_dict(), BEST_FEDERATED_MODEL_PATH)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Model name: Federated Average | Epoch time (Federated Training): {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "print(\"Federated Model training complete.\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: Baseline | Test Loss: 0.004 | Test Acc: 80.95%\n",
      "Model name: Federated Average | Test Loss: 0.006 | Test Acc: 74.20%\n"
     ]
    }
   ],
   "source": [
    "# The main testing loop\n",
    "# Load the model\n",
    "baseline_model.load_state_dict(torch.load(BASELINE_MODEL_PATH))\n",
    "federated_model.load_state_dict(torch.load(BEST_FEDERATED_MODEL_PATH))\n",
    "\n",
    "baseline_test_loss, baseline_test_acc = ms.test(baseline_model, test_loader, criterion, device=device)\n",
    "fed_avg_test_loss, fed_avg_test_acc = ms.test(federated_model, test_loader, criterion, device=device)\n",
    "\n",
    "print(f'Model name: Baseline | Test Loss: {baseline_test_loss:.3f} | Test Acc: {baseline_test_acc*100:.2f}%')\n",
    "print(f'Model name: Federated Average | Test Loss: {fed_avg_test_loss:.3f} | Test Acc: {fed_avg_test_acc*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}