{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code cell intended for imports and global settings.\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from SupportClasses import ModelSupport as ms, EnvironmentSetup as env\n",
    "\n",
    "# Define main data directory\n",
    "DATA_DIR = 'tiny-imagenet-200' # Original images come in shapes of [3,64,64]\n",
    "\n",
    "# Define training and validation data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VALID_DIR = os.path.join(DATA_DIR, 'val')\n",
    "\n",
    "# Global values\n",
    "NUMBER_OF_NODES = 3     # We only support values between 2 and 4 at present.\n",
    "NUMBER_OF_EPOCHS = 5\n",
    "NUMBER_OF_CLASSES = NUMBER_OF_NODES     # Generally we will have the same number of classes as nodes.\n",
    "PATH = 'baseline-model.pt'\n",
    "\n",
    "# Model specific values\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "The classes in the training and testing set are ['airplane', 'automobile', 'bird']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-2090efe0e722>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;31m# Data distributions based on the number of nodes.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mdata_distribution_list\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_distribution\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mNUMBER_OF_NODES\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mtrain_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclasses\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdownload_cifar10\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mNUMBER_OF_CLASSES\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;31m# Now we distribute the dataset, for each node.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\git\\CSI5340-GroupProject\\SupportClasses\\EnvironmentSetup.py\u001B[0m in \u001B[0;36mdownload_cifar10\u001B[1;34m(num_of_classes, train_batch_size, test_batch_size, num_workers)\u001B[0m\n\u001B[0;32m    109\u001B[0m                                          download=True, transform=transform)\n\u001B[0;32m    110\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 111\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mfilter_data_by_classes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_of_classes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrainset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtestset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    112\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\git\\CSI5340-GroupProject\\SupportClasses\\EnvironmentSetup.py\u001B[0m in \u001B[0;36mfilter_data_by_classes\u001B[1;34m(num_of_classes, trainset, testset)\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[0mtrain_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtarget\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mclasses\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mtarget\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtrainset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtargets\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[0mtest_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtarget\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mclasses\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mtarget\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtestset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtargets\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 41\u001B[1;33m     \u001B[0mtrainset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtargets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtestset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtargets\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrainset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtargets\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtrain_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtestset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtargets\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtest_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     42\u001B[0m     \u001B[0mtrainset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtestset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrainset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtrain_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtestset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtest_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "# Create separate validation subfolders for the validation images based on\n",
    "# their labels indicated in the val_annotations txt file\n",
    "val_img_dir = os.path.join(VALID_DIR, 'images')\n",
    "\n",
    "# Open and read val annotations text file\n",
    "fp = open(os.path.join(VALID_DIR, 'val_annotations.txt'), 'r')\n",
    "data = fp.readlines()\n",
    "\n",
    "# Create dictionary to store img filename (word 0) and corresponding\n",
    "# label (word 1) for every line in the txt file (as key value pair)\n",
    "val_img_dict = {}\n",
    "for line in data:\n",
    "    words = line.split('\\t')\n",
    "    val_img_dict[words[0]] = words[1]\n",
    "fp.close()\n",
    "\n",
    "# Display first 10 entries of resulting val_img_dict dictionary\n",
    "{k: val_img_dict[k] for k in list(val_img_dict)[:10]}\n",
    "\n",
    "# Create subfolders (if not present) for validation images based on label,\n",
    "# and move images into the respective folders\n",
    "for img, folder in val_img_dict.items():\n",
    "    newpath = (os.path.join(val_img_dir, folder))\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    if os.path.exists(os.path.join(val_img_dir, img)):\n",
    "        os.rename(os.path.join(val_img_dir, img), os.path.join(newpath, img))\n",
    "\n",
    "# Define transformation sequence for image pre-processing\n",
    "# If not using pre-trained model, normalize with 0.5, 0.5, 0.5 (mean and SD)\n",
    "# If using pre-trained ImageNet, normalize with mean=[0.485, 0.456, 0.406],\n",
    "# std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preprocess_transform_pretrain = T.Compose([\n",
    "                T.Resize(256), # Resize images to 256 x 256\n",
    "                T.CenterCrop(224), # Center crop image\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),  # Converting cropped images to tensors\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This code cell will be used for setting up the unbalanced datasets.\n",
    "# Define device to use (CPU or GPU). CUDA = GPU support for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# Note that we are implicitly assuming the data is well balanced in the original dataset.\n",
    "# Data distributions based on the number of nodes.\n",
    "data_distribution_list = env.data_distribution(NUMBER_OF_NODES)\n",
    "# Define batch size for DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoaders for pre-trained models (normalized based on specific requirements)\n",
    "train_loader_pretrain = env.generate_dataloader(TRAIN_DIR, \"train\",\n",
    "                                  transform=preprocess_transform_pretrain, use_cuda=use_cuda, batch_size=batch_size)\n",
    "\n",
    "val_loader_pretrain = env.generate_dataloader(val_img_dir, \"val\",\n",
    "                                 transform=preprocess_transform_pretrain, use_cuda=use_cuda, batch_size=batch_size)\n",
    "\n",
    "# Now we distribute the dataset, for each node.\n",
    "unbalanced_training_sets = []\n",
    "for data_dist in data_distribution_list:\n",
    "    unbalanced_training_sets.append( env.unbalance_training_set(train_set=train_set, classes=classes, data_distribution=data_dist) )\n",
    "\n",
    "print(\"Done loading data.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This code cell is likely where you will want to do the GAN work on the given datasets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "ConvNet(\n  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (fc1): Linear(in_features=1568, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=3, bias=True)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_train_loader = env.create_single_loader(train_set.dataset)\n",
    "# This code cell is to be used for importing data and setting up the model.\n",
    "training_loaders, validation_loader, test_loader = env.create_data_loaders(training_sets=unbalanced_training_sets,\n",
    "                                                                       validation_set=validation_set, test_set=test_set)\n",
    "# Create and load the models. We initiate the model with None as we will update it with the global model in each round.\n",
    "fed_models = {f\"Federated_Model_{i+1}\": ms.FederatedModel(train_loader, validation_loader,\n",
    "                                                          ms.ConvNet(NUMBER_OF_CLASSES))\n",
    "                for i, train_loader in enumerate(training_loaders)}\n",
    "\n",
    "# Create the baseline, non-federated model.\n",
    "baseline_model = ms.ConvNet(NUMBER_OF_CLASSES)\n",
    "# Create the federated model\n",
    "federated_model = ms.ConvNet(NUMBER_OF_CLASSES)\n",
    "\n",
    "# Send the models to the CUDA device if it exists.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "baseline_model.to(device=device)\n",
    "federated_model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model training complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we train a baseline model on all data. No federation as our baseline.\n",
    "optimizer = optim.Adam(baseline_model.parameters())\n",
    "\n",
    "# We train a new model, if the model does not already exist in memory.\n",
    "if not os.path.exists(PATH):\n",
    "    for epoch in range(NUMBER_OF_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = ms.train(baseline_model, global_train_loader, optimizer, criterion, device=device)\n",
    "        valid_loss, valid_acc = ms.test(baseline_model, validation_loader, criterion, device=device)\n",
    "        end_time = time.time()\n",
    "        # Get the time to perform non-federated learning\n",
    "        epoch_mins, epoch_secs = ms.epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Model name: Baseline Model | Epoch time (Baseline Training): {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    torch.save(baseline_model.state_dict(), PATH)\n",
    "print(\"Baseline model training complete.\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.03947 | Train Acc: 64.00%\n",
      "\t Val. Loss: 0.00791 |  Val. Acc: 79.66%\n",
      "Epoch: 01 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.03828 | Train Acc: 50.00%\n",
      "\t Val. Loss: 0.00828 |  Val. Acc: 49.41%\n",
      "Epoch: 01 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.03981 | Train Acc: 42.00%\n",
      "\t Val. Loss: 0.00908 |  Val. Acc: 34.01%\n",
      "Epoch: 01 | Model name: Federated Average | Epoch time (Federated Training): 0m 9s\n",
      "\t Val. Loss: 0.00844 |  Val. Acc: 77.30%\n",
      "Epoch: 02 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.02441 | Train Acc: 82.00%\n",
      "\t Val. Loss: 0.00324 |  Val. Acc: 94.99%\n",
      "Epoch: 02 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.02649 | Train Acc: 78.00%\n",
      "\t Val. Loss: 0.00370 |  Val. Acc: 95.49%\n",
      "Epoch: 02 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.02370 | Train Acc: 83.00%\n",
      "\t Val. Loss: 0.00359 |  Val. Acc: 94.41%\n",
      "Epoch: 02 | Model name: Federated Average | Epoch time (Federated Training): 0m 8s\n",
      "\t Val. Loss: 0.00333 |  Val. Acc: 96.49%\n",
      "Epoch: 03 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.01663 | Train Acc: 86.00%\n",
      "\t Val. Loss: 0.00289 |  Val. Acc: 93.88%\n",
      "Epoch: 03 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.01461 | Train Acc: 84.00%\n",
      "\t Val. Loss: 0.00273 |  Val. Acc: 90.40%\n",
      "Epoch: 03 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.01266 | Train Acc: 89.00%\n",
      "\t Val. Loss: 0.00276 |  Val. Acc: 90.12%\n",
      "Epoch: 03 | Model name: Federated Average | Epoch time (Federated Training): 0m 9s\n",
      "\t Val. Loss: 0.00212 |  Val. Acc: 95.70%\n",
      "Epoch: 04 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.00878 | Train Acc: 95.00%\n",
      "\t Val. Loss: 0.00168 |  Val. Acc: 94.70%\n",
      "Epoch: 04 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.00828 | Train Acc: 95.00%\n",
      "\t Val. Loss: 0.00156 |  Val. Acc: 96.10%\n",
      "Epoch: 04 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.01172 | Train Acc: 86.00%\n",
      "\t Val. Loss: 0.00183 |  Val. Acc: 95.45%\n",
      "Epoch: 04 | Model name: Federated Average | Epoch time (Federated Training): 0m 9s\n",
      "\t Val. Loss: 0.00130 |  Val. Acc: 97.10%\n",
      "Epoch: 05 | Model name: Federated_Model_1\n",
      "\tTrain Loss: 0.00890 | Train Acc: 91.00%\n",
      "\t Val. Loss: 0.00334 |  Val. Acc: 86.00%\n",
      "Epoch: 05 | Model name: Federated_Model_2\n",
      "\tTrain Loss: 0.00427 | Train Acc: 98.00%\n",
      "\t Val. Loss: 0.00173 |  Val. Acc: 94.56%\n",
      "Epoch: 05 | Model name: Federated_Model_3\n",
      "\tTrain Loss: 0.00636 | Train Acc: 94.00%\n",
      "\t Val. Loss: 0.00187 |  Val. Acc: 92.70%\n",
      "Epoch: 05 | Model name: Federated Average | Epoch time (Federated Training): 0m 9s\n",
      "\t Val. Loss: 0.00177 |  Val. Acc: 93.23%\n",
      "Federated Model training complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we train our federated model.\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    # Perform the computation steps on the individual models\n",
    "    start_time = time.time()\n",
    "    for key, fed_model in fed_models.items():\n",
    "        # Update each model with the global model, before training again.\n",
    "        fed_model.model.load_state_dict(federated_model.state_dict())\n",
    "        fed_model.model.to(device=device)\n",
    "\n",
    "        # Begin training\n",
    "        optimizer = optim.Adam(fed_model.model.parameters())\n",
    "        train_loss, train_acc = ms.train(fed_model.model, fed_model.train_loader, optimizer, criterion, device=device)\n",
    "        valid_loss, valid_acc = ms.test(fed_model.model, fed_model.validation_loader, criterion, device=device)\n",
    "        print(f'Epoch: {epoch+1:02} | Model name: {key}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    end_time = time.time()\n",
    "    # Get the time to perform federated learning\n",
    "    epoch_mins, epoch_secs = ms.epoch_time(start_time, end_time)\n",
    "\n",
    "    # Average the federated models and combine their weights into the main model.\n",
    "    federated_model.load_state_dict(ms.federated_averaging(fed_models))\n",
    "    # Validate this model on a, small balanced validation set\n",
    "    valid_loss, valid_acc = ms.test(federated_model, validation_loader, criterion, device=device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # This will save our best model in case we encounter a drop off during training.\n",
    "        torch.save(federated_model.state_dict(), 'best-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Model name: Federated Average | Epoch time (Federated Training): {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "print(\"Federated Model training complete.\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: Baseline | Test Loss: 0.000 | Test Acc: 99.90%\n",
      "Model name: Federated Average | Test Loss: 0.000 | Test Acc: 99.94%\n"
     ]
    }
   ],
   "source": [
    "# The main testing loop\n",
    "# Load the model\n",
    "baseline_model.load_state_dict(torch.load(PATH))\n",
    "federated_model.load_state_dict(torch.load('best-model.pt'))\n",
    "\n",
    "baseline_test_loss, baseline_test_acc = ms.test(baseline_model, test_loader, criterion, device=device)\n",
    "fed_avg_test_loss, fed_avg_test_acc = ms.test(federated_model, test_loader, criterion, device=device)\n",
    "\n",
    "print(f'Model name: Baseline | Test Loss: {baseline_test_loss:.3f} | Test Acc: {baseline_test_acc*100:.2f}%')\n",
    "print(f'Model name: Federated Average | Test Loss: {fed_avg_test_loss:.3f} | Test Acc: {fed_avg_test_acc*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}